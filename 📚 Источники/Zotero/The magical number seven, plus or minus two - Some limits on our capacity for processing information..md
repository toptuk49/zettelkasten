> [!info]+ Metadata
> **Title**:: The magical number seven, plus or minus two: Some limits on our capacity for processing information.
>
> **Author**:: George A. Miller; 
> **Year**:: 1994
>
> **Item Type**:: Journalarticle
> **Publisher**: 
> **Pages**:: 
>
> *Read start*::
> *Read end*::
> 
> **Tags**:: `` #source/zotero
> **Keywords**:: 
> **URL**:: https://doi.apa.org/doi/10.1037/0033-295X.101.2.343

> [!link]+ Zotero Link
>[PDF](zotero://select/library/items/638M232Y)

> [!abstract]+
> 

### Highlights
>[!Annotation|#ffd400]+
>#### ^7AR49SZ7
>*« The "amount of information" is exactly the same concept that we have talked about for years under the name of "variance." The equations are different, but if we hold tight to the idea that anything that increases the variance also increases the amount of information we cannot go far astray »* ([Page ](zotero://open-pdf/library/items/638M232Y?page=&annotation=7AR49SZ7))
>
>Количество информации и дисперсию (в данном случае непредсказуемость) можно считать одной и той же идеей с незначительной погрешностью.

>[!Annotation|#ffd400]+
>#### ^ZXECIY86
>*« The advantages of this new way of talking about variance are simple enough. Variance is always stated in terms of the unit of measurement -- inches, pounds, volts, etc. -- whereas the amount of information is a dimensionless quantity »* ([Page ](zotero://open-pdf/library/items/638M232Y?page=&annotation=ZXECIY86))
>
>Рассматривая количество информации как непредсказуемость, можно представлять её как безразмерную величину.

>[!Annotation|#ffd400]+
>#### ^ENKFAECY
>*« The similarity of variance and amount of information might be explained this way: When we have a large variance, we are very ignorant about what is going to happen. If we are very ignorant, then when we make the observation it gives us a lot of information. On the other hand, if the variance is very small, we know in advance how our observation must come out, so we get little information from making the observation »* ([Page 2](zotero://open-pdf/library/items/638M232Y?page=2&annotation=ENKFAECY))
>
>Чем более непредсказуемый результат мы получаем от наших действий, тем большее количество информации мы получаем и наоборот.

>[!Annotation|#ffd400]+
>#### ^RFM4EHE6
>*« Now just a brief word about the bit and we can begin to look at some data. One bit of information is the amount of information that we need to make a decision between two equally likely alternatives. If we must decide whether a man is less than six feet tall or more than six feet tall and if we know that the chances are 50-50, then we need one bit of information. Notice that this unit of information does not refer in any way to the unit of length that we use -- feet, inches, centimeters, etc. However you measure the man's height, we still need just one bit of information.  Two bits of information enable us to decide among four equally likely alternatives. Three bits of information enable us to decide among eight equally likely alternatives. Four bits of information decide among 16 alternatives, five among 32, and so on. That is to say, if there are 32 equally likely alternatives, we must make five successive binary decisions, worth one bit each, before we know which alternative is correct. So the general rule is simple: every time the number of alternatives is increased by a factor of two, one bit of information is added »* ([Page 2](zotero://open-pdf/library/items/638M232Y?page=2&annotation=RFM4EHE6))
>
>Чем больше становится вариантов выбора, тем большим количеством информации нам нужно обладать, чтобы принять решение.

>[!Annotation|#ffd400]+
>#### ^DIC3QKP9
>*« What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory »* ([Page 13](zotero://open-pdf/library/items/638M232Y?page=13&annotation=DIC3QKP9))
>
>Миллер полагает, что семь - это число достаточно близкое к измерению единиц информации, которые мы одновременно можем держать в голове.

